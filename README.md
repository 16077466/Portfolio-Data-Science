# Portfolio Minor Applied Data Sciences  
**Firstname** : Ramon  
**Lastname** : van der Elst   
**Student number**: 16077466   
**Date**: 04 February 2022  
**Project group** : IMPutation  


# Table of Contents
=================

  - [Portfolio Minor Applied Data Sciences](#portfolio-minor-applied-data-sciences)
  - [DataCamp Course](#datacamp-course)
  
  - [Reflection and evaluation STARR-based](#reflection-and-evaluation-starr-based)
    - [Reflection on own contribution to the project](#reflection-on-own-contribution-to-the-project)
    - [Reflection on own learning objectives](#reflection-on-own-learning-objectives)
    - [Evaluation on te group project as a whole](#evaluation-on-the-group-project-as-a-whole)
  
  - [**Subject #1:** Research project](#subject-1-research-project)
  - [**Subject #2:** Domain knowledge](#subject-2-domain-knowledge)
  - [**Subject #3:** Communication](#subject-3-communication)
    - [Presentations](#presentations)
    - [Writing paper](#writing-paper)
  
# DataCamp Course
* By my own mistake I had been learning Python through the datascience server with the inpro assignments. Therefore I do not have datacamp course completed.

# Reflection and evaluation STARR-based
## Reflection on own contribution to the project
* Situation: At the end of the project I had a hard time dealing with the way the project had gone thus far. Apart from me liking to work with project team, certain ways we handled the project did not sit right with me and can be put on me as well as the rest. For example there was a lot of research done on former studies about Building management system time series data imputation and different imputation methods known to work well for time series data. But in the end this literature did not get taken seriously and everyone did their own work without having research to back it up. This led to problems later on and became apparent during the writing on the research paper. 
* Task: My task I had in this context was literature researcher and writing the research paper together with my project collogue Julien. Here we had to collect all the data and results from the imputation methods used and tested by our other group members and to make the research valid we had to link the methods and results to former literature/research. Since everything was done without the backed-up research before hand, this caused for a lot of extra work going through dozens of researches and eventually taking two whole weeks to validate all research done.
* Action: How I handled this situation was by extensively going through my researches found earlier in the project and going online finding new valid research. There was no time and point staying stuck in the fact that the methods were not handled correct by the project members and me so I did what I had to do at that given time and that was put a lot of overwork in to make sure the research paper had all the research it needed.  
* Result: In the end the extra work and over hours that were put in the research paper paid off very well. The research paper got finished on time and with sufficient and valid research to back up our findings and methods used during this project.
* Reflection: The main thing I learned from this situation is that I could have been more assertive at the beginning of the project and said to my team “Listen, I am the one who does the literature research for former research done on this topic and field. It is okay if you do your own research too but make sure to note the research you found or want to use and link it to me for later use.” This would have saved all the extra work and trouble I had to go through. 

## Reflection on own learning objectives
* Situation: When I started this minor Applied Data Science, I had certain goals for myself. Since I have no background in programming other than 2 basic courses I wanted to challenge myself and see how Applied Data science would go. The first in C++ for my first year of informatica and the second in Java for the first year of HBO-ICT here in The Hague. I did not want to learn more on the field of programming since I knew that is not what I want to do later for my work but I wanted to gain more insight and knowledge when it comes to the techniques used in data science. In the project I worked in this translated to knowing more about methods used for the imputation of missing data in small or big data sets. The coding behind this was not my goal but the techniques themselves, how they work, when they are used and how to for example optimize them properly. 
* Task: My task in this was to learn more about the field of data science and again specifically for my project the imputation methods and the theory behind them. What I had to do was extensive research on former studies and research that have been done on the subject ‘Missing data imputation for building management system time series data.’. Apart from studying these researches I had to write about them in the research proposal,  research paper and my personal file where I saved all the researches I found with a small summary and what the research can be used for. (see 4.2 literature review) 
* Action: What I did was the extensive research on former studies and research that have been done on the subject ‘Missing data imputation for building management system time series data.’. I had to validate these studies with my project team and problem owner Baldiri. Apart from studying these researches I had wrote about them in the research proposal,  research paper and my personal file where I saved all the researches I found with a small summary and what the research can be used for. 
* Result: The results of my work were that I know a lot more about the subject and field than I thought I would but also that I confirmed my uninterest in programming. This might seem weird since that is a part of data science, but ahead of time I knew there would be project groups where not everyone would have to do the same work and there will be members of the team working on programming because it is their strong side, and for me my strong side and interests lie in data visualization, data analyzation and IT skills like data management.
* Reflection: I am still very happy with what I did and what I have learned during this minor. I might not always be satisfied with the way it went or with parts I maybe should have dove into more. I still take away a lot from this minor even if it is the fact that I know I don’t want to pursue a future in Applied Data Science.

## Evaluation on the group project as a whole
* Situation: The group project as a whole I am very satisfied with. From the good cooperation with my project team and project owner to the project I worked on. I am very fortunate to have worked on the IMPutation project as I thought from the start that this was the most interesting and fun project of them all. The method we used to work on this project made it a smooth and organized project at the start and got a little less smooth later on but that had other reasons that I do not know of. One of the key factors why this project was so fun to me is the problem owner Baldiri. He always brought a good atmosphere, instructive feedback and made sure the entire project team delivered week by week. I can honestly say that without Baldiri this project would have been a lot different for me.
* Task: The task in this project was to write guidelines for data imputation and which methods work best for different data measurements, size of the gap that needs to be imputed and data classification. My task in this project was to do research, write about the research in the research proposal and paper. The paper with the guidelines eventually was going to be submitted to a conference called CLIMA. 
* Action: What I did was the extensive research on former studies and research that have been done on the subject ‘Missing data imputation for building management system time series data.’. I had to validate these studies with my project team and problem owner Baldiri. Apart from studying these researches I had wrote about them in the research proposal,  research paper and my personal file where I saved all the researches I found with a small summary and what the research can be used for. 
* Result: The result for the project as a whole is the research paper we wrote. It got submitted to CLIMA and at the moment of writing this we are waiting to hear if it got published or not. I would say the project was a success and I am very happy and satisfied with the result and my project team for achieving this together.
* Reflection: I learned not only a lot more about the data science field but also about writing research papers, methods and techniques for data imputation. Next time I would dive more into programming just to learn more on the field where I lack skills. I know I don’t want to pursue a career in that field but I would still not be against learning more. 

[Back to table of contents](#table-of-contents)

# **Subject #1** Research project
## Task definition
Context: 
This research project was done because the research group Energy In Transition (EiT) wanted to have research done on data imputation and the possibilities of guidelines on it. Specifically, they wanted guidelines for data imputation in building management system time series data, that would be applicable on a global scale.  The research group EiT had already done multiple researches when it comes to data imputation but never written guidelines for it thus, the research question: “Which imputation techniques should be applied for data imputation in building energy time series data?” was formulated.  	But why would this research be important you might ask. One reason is the fact that missing data can lead to bias when it comes to decision making in companies. If a lets say a company is trying to forecast/predict future trends based on data that is missing 10-20% of its data, can we say the forecast/prediction will be accurate enough to base a data-driven strategy on it? Companies in almost any case want to have complete datasets or as complete as it can be to have a high accuracy when it comes to working with the data, if that is forecasting, decision making or even selling data it always needs to be complete.

The main research question and the sub-questions:
-	(Main) Which imputation techniques should be applied for data imputation in building management system time series data?

Based on the task given by EiT we did our research and came to multiple versions of a research question. In the end we decided on the question above. The important part of the question was the focus on the eventual goal, the guidelines. The second important part was to narrow down which group this should be applied to thus the part “for data imputation in building management system time series data.” In the end the word “should” is important since it refers to the guidelines where we simply want to give a guide on methods that should be used in global terms when imputing missing.

-	What imputation methods are known for imputing time series data?

This sub-question was chosen because we thought it would be important to start doing research about the known imputation methods that are out there as well as to get a better understanding on what we would have to try. When it comes to trying I imply imputing data and write guidelines about the methods.

-	Which imputation techniques are best suited for what gap sizes?

After reviewing multiple imputation techniques we came to the conclusion that imputing missing data, in time series data, is largely defined by the amount of data that needs to be imputed at a time or by the size the gap in the data. Certain methods work better with smaller gaps (e.g. 0 minutes to 60 minutes) where other methods work very well with larger gaps (e.g. 1 day to 5 days). For this reason we thought it would be essential to group the best methods for different gap sizes.

-	What imputation techniques are best suited for which types of data?

Same as the different gap sizes, types of data is an important factor when it comes to imputing missing data since different data types can decide which methods works best. The types of data we refer to are: Nominal, ratio and interval. The research we did already gave us the idea/conclusion that there wont be a single best imputation method for all types of data. Our results also confirmed this since the method Hot Deck and Recurrent Neural Network (RNN)shared the spot for best performance. Hot deck worked better for Nominal and ration data where RNN worked better for interval data.

## 3.2 Evaluation
What can be done for future research on this subject:

•	Our research had a relatively large focus on evaluating  imputation with metrics based on the error. Therefore we would recommend a different focus for future research. This is to focus more on the impact of forecasting using imputed data. This could be done for weather data using machine learning algorithms to evaluate the prediction error of methods.

•	Referring back to types of data we also came to the conclusion that the datasets we used only contained numerical data and no ordinal data. So to get a complete view on the performance of the methods tested there needs to be more research done about the different types of data with the methods we used. 

•	We used a gated recurrent unit (GRU) recurrent neural network (RNN) that in the end had some limitations because of the way we set it up. Therefor we believe the full potential for this method has not been achieved yet and could be looked into more for future research since it already showed very promising results. To achieve this the RNN architecture should be changed to an encoder-decoder sequential based design. This should remove the bias of imputing its own imputed values.

## 3.3 Conclusion
Based on the research we did, the following conclusions can be made:
-	Performance was evaluated using both root mean squared error (RMSE) and variance error (VE) as can be seen in figure 1. However, our RMSE did not perform as we expected. Since the point was to impute trends in data back and combining this with the VE score we got we can conclude that a good start has been made for this research. 

-	Going to the goal of the research, the guidelines we wrote were based of off 4 methods we tested and evaluated. Those methods are Last Observation Carried Forward (LOCF), K nearest neighbor (KNN), Hot deck (HD) and Gated Recurrent Unit (GRU) Recurrent Neural Network (RNN). And for our guideline we made conclusions based on the method that performs best per data type: Nominal, Ratio and Interval as can be seen in table 1. We can conclude that HD performs the best for both nominal and ratio data and RNN performs the best on interval data, based on our findings. 

One side note that we have to make is that HD performed this well because this research used a large amount of similar data units. When there would be a dataset with no similar data HD would not be useful and even not useable. 

## 3.4 Planning
At the start of the project we decided to structure of the 16 weeks we had ahead of us. First we decided to have sprints of 2-3 weeks, this way we had enough time to make progress every sprint and also keep a natural flow since we have an ongoing deadline every 2-3 weeks that supports a better outcome of the project. Next we had daily online or in person stand-up meetings where everyone got the opportunity to explain what they were going to work on for that day.  At the end of the sprint we did a short version of a retrospective where we reviewed the work that was done and gave everyone feedback on their work they did.  In this retrospective we also created the new sprint for the next 2-3 weeks and added ongoing issues as well as new issues. The issues got assigned to the person working on them in Jira. One disclaimer here is that Jira only allows 1 assignee and this can result in no evidence of a person having worked on- or helped to complete a task. However to prove we used the Scrum method well and update our Jira scrum board every sprint, screen shots have been taken of the sprint issues and can be seen in figures 2-7.

[Back to table of contents](#table-of-contents)

# **Subject #2** Domain Knowledge
## Introduction to the subject field
Missing data in datasets can be seen as a well known issue for companies. Missing data can lead to many problems such as creating bias for decision making, misleading forecasts if incomplete data has been used and much more. Therefore, missing data need to be imputed with values that are reasonable given the context. Data imputation is a big topic where researches have been done for a longer time now. But still to this day there is a lot more that can be done when it comes to learning more about data imputation, and in this specific case data imputation for time series data. Time series imputation is a challenging subject due to the existence of non-linear dependencies between current and past values. Simpler imputation methods, such as deletion of data rows containing missing values or filling gaps in data with the last observed value, add bias to the data and are not very efficient. There is never a situation where one imputation method works best for all cases and that is one of many reasons why data imputation can be a complicated issue.	Many methods are known to work well when imputing missing data in time series data, such as: Interpolation, K nearest neighbor (KNN), Last Observation Carried forward (LOCF), Linear regression types of neural networks such as a Recurrent Neural network. What is important about data imputation and the many methods is how you test/evaluate them to score their performance. Results can show that a certain method outperforms other methods by having the lowest variance error (VE) or Root mean squared error (RMSE). These are just 2 evaluation metrics that can be used to evaluate performance. There will be many challenges to face when imputing missing data such as understanding commonness and patterns of missing data, selecting the right imputation methods and how to evaluate performance of the selected imputation methods.

## Literature review
My work for literature review can be found in the latter link:
[Literature review](https://github.com/16077466/Portfolio-Data-Science/blob/4c79a2c69dc6f24b3d03b91e1e69979c35fb56d4/Research%20Paper/Literature%20review%20Imputation%20done%20by%20Ramon.pdf)

The literature review selected consists solely out of literature research that was studied and evaluated for reference and further use in this research.

## Explanation of terminology, Jargon and definitions

|Term| Description|
|:------:|--------:|
|Building management systems| (BMS)	A computer based control system that is installed to regulate electrical and mechanical equipment such as power system, lightning and ventilation.|
|Time series data|	Series of data points indexed in timely order.|
|Imputation|	In statistics, imputation is the process of replacing missing data with replaced values|
|Machine learning|	A broad research field within artificial intelligence, concerned with the development of algorithms and techniques that enable computers to learn.|
|Neural network|	A series of algorithms that tries to recognize underlying relationships in a set of data through a process that copies the way the human brain operates.|
|Scrum Retrospective|	A standard part of the Scrum framework. At the end of a sprint, the team reflects - independently of the content - on the team process, their way of working, and the relationships between them.|
|Variance error|	Error variance usually indicates how much random fluctuation is expected within scores and often forms part of the denominator of test statistics|
|Root mean squared error|	A widely used measure of the differences between values predicted by a model or estimator and observed values.|

[Back to table of contents](#table-of-contents)

# **Subject #3:** Communication

## Presentations
The internal presentations were done together with Jesús Martínez de Juan and sadly lost the presentation that was done on the 22-11-21 since we used the same file on Canva for the later presentations and this overwrote the file before we thought of saving a separate one.

**Internal presentations:**  

- [Internal presentation week 4](/Presentations/Imputation%20internal%20presentation%20week%204%2020-09-21.pdf)
- [Internal presentation 12-11-21](Presentations/Imputation%20Project%20team%20Internal%20presentation%2012-11-21.pdf)


Both the external presentations I prepared and presented. I have my speaker notes for the presentation on 08-10-21 which I presented. As for the one on 12-11-21 that one got prepared as a group and I presented together with Albert Corson and Adrien Lucbert.

**External presentations:**
- [External presentation week 6](Presentations/Team%20IMP%20-%20external%20presentation%20week6.pdf)
- [External presentation 08-11-21 Notes](/Presentations/08-11-21%20Ramon%20External%20presentation%20IMP.pdf)

**Final presentation**
- [Final presentation open team IMP](/Presentations/Final_Imputation%20Project%20team%20open%20presentation.pdf)
  - [Final presentation open Ramon Notes](Presentations/Notes_Final_presentation_IMP_Ramon.pdf)


## Writing paper

Research proposal
The research proposal was written by me in collaboration with Julien van der Niet. 

- [Research proposal Applied Data Science IMP](https://github.com/16077466/Portfolio-Data-Science/blob/15dfd6a9a4b17704e57e3393f31b512b13770176/Research%20Paper/Research%20proposal%20Applied%20Data%20Science%20project%20IMP.pdf)

Research paper
The research paper has been written mainly by Julien van der Niet. Here I was the main help and made sure all added information was confirmed, supported by literature research, correct grammar and written inline with what CLIMA conference asked. The references to literature in the paper have been written by me and Julien as well as found by both me and Julien.

[Research paper PDF](https://github.com/16077466/Portfolio-Data-Science/blob/7515316d4b0968d8a263f0f5b2f48b7a5d09cb1b/Research%20Paper/Research%20paper%20Version%201-9-2022%20IMP.docx.pdf)

[Research paper LaTeX](https://github.com/16077466/Portfolio-Data-Science/blob/e2f91f76a1a2790c8724c95c97f45e0a2e8b4d1a/Research%20Paper/IMP%20Final%20Research%20Paper%20LaTeX%20version%20(1).pdf)

Researches like got researched and validated by me and written by Julien. After it was written I would make adjustments where needed: 

“HD can be outperformed by machine learning as seen in (Sree Dhevi, 2014) [1] but it is applicable when there are similar units available for study. The time series imputation performance of different types of RNN’s has been studied before in Che et al. (2018) [2]. The study concluded that when a Gated Recurrent Units (GRU) architecture is properly set up “it pulled significantly ahead of non-deep learning methods” “

“Pazhoohesh et al. (2019) [3] found that for datasets where 10% to 30 % of the data is missing, the KNN algorithm does great compared to eight other methods. Poloczek et al. 2014 [4] analysed the use of KNN regression and LOCF and found that both did well for the study, but that KNN regression outperformed other methods.”




[Back to table of contents](#table-of-contents)

